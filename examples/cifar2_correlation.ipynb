{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "e1699de3-2a3b-4848-bad7-93cd265712d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "from scipy.stats import spearmanr\n",
    "import wget\n",
    "import numpy as np\n",
    "import torch as ch\n",
    "import torchvision\n",
    "\n",
    "from trak import TRAKer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fa6e5bb6-9729-43bc-af8d-6b11a1aa4bfd",
   "metadata": {},
   "source": [
    "# Initialize model and dataloaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "6a3e7927-cdab-411e-8a19-f799ab41f378",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    from ffcv.fields.decoders import IntDecoder, SimpleRGBImageDecoder\n",
    "    from ffcv.loader import Loader, OrderOption\n",
    "    from ffcv.pipeline.operation import Operation\n",
    "    from ffcv.transforms import RandomHorizontalFlip, Cutout, \\\n",
    "        RandomTranslate, Convert, ToDevice, ToTensor, ToTorchImage\n",
    "    from ffcv.transforms.common import Squeeze\n",
    "except:\n",
    "    print('No ffcv installed')\n",
    "\n",
    "BETONS = {\n",
    "        'train': \"/path/to/train/set\",\n",
    "        'val': \"/path/to/val/set\",\n",
    "}\n",
    "\n",
    "STATS = {\n",
    "        'mean': [125.307, 122.961, 113.8575],\n",
    "        'std': [51.5865, 50.847, 51.255]\n",
    "}\n",
    "\n",
    "def get_dataloader(batch_size=256,\n",
    "                   num_workers=8,\n",
    "                   split='train',  # split \\in [train, val]\n",
    "                   aug_seed=0,\n",
    "                   should_augment=True,\n",
    "                   indices=None):\n",
    "        label_pipeline: List[Operation] = [IntDecoder(),\n",
    "                                           ToTensor(),\n",
    "                                           ToDevice(ch.device('cuda:0')),\n",
    "                                           Squeeze()]\n",
    "        image_pipeline: List[Operation] = [SimpleRGBImageDecoder()]\n",
    "\n",
    "        if should_augment:\n",
    "                image_pipeline.extend([\n",
    "                        RandomHorizontalFlip(),\n",
    "                        RandomTranslate(padding=2, fill=tuple(map(int, STATS['mean']))),\n",
    "                        Cutout(4, tuple(map(int, STATS['std']))),\n",
    "                ])\n",
    "\n",
    "        image_pipeline.extend([\n",
    "            ToTensor(),\n",
    "            ToDevice(ch.device('cuda:0'), non_blocking=True),\n",
    "            ToTorchImage(),\n",
    "            Convert(ch.float32),\n",
    "            torchvision.transforms.Normalize(STATS['mean'], STATS['std']),\n",
    "        ])\n",
    "        \n",
    "        beton_url = BETONS[split]\n",
    "        beton_path = f'./{split}.beton'\n",
    "        wget.download(beton_url, out=str(beton_path), bar=None)\n",
    "        \n",
    "        return Loader(beton_path,\n",
    "                      batch_size=batch_size,\n",
    "                      num_workers=num_workers,\n",
    "                      order=OrderOption.SEQUENTIAL,\n",
    "                      drop_last=False,\n",
    "                      seed=aug_seed,\n",
    "                      indices=indices,\n",
    "                      pipelines={'image': image_pipeline, 'label': label_pipeline})\n",
    "\n",
    "# Resnet9\n",
    "class Mul(ch.nn.Module):\n",
    "    def __init__(self, weight):\n",
    "        super(Mul, self).__init__()\n",
    "        self.weight = weight\n",
    "    def forward(self, x): return x * self.weight\n",
    "\n",
    "\n",
    "class Flatten(ch.nn.Module):\n",
    "    def forward(self, x): return x.view(x.size(0), -1)\n",
    "\n",
    "\n",
    "class Residual(ch.nn.Module):\n",
    "    def __init__(self, module):\n",
    "        super(Residual, self).__init__()\n",
    "        self.module = module\n",
    "    def forward(self, x): return x + self.module(x)\n",
    "\n",
    "\n",
    "def construct_rn9(num_classes=2):\n",
    "    def conv_bn(channels_in, channels_out, kernel_size=3, stride=1, padding=1, groups=1):\n",
    "        return ch.nn.Sequential(\n",
    "                ch.nn.Conv2d(channels_in, channels_out, kernel_size=kernel_size,\n",
    "                            stride=stride, padding=padding, groups=groups, bias=False),\n",
    "                ch.nn.BatchNorm2d(channels_out),\n",
    "                ch.nn.ReLU(inplace=True)\n",
    "        )\n",
    "    model = ch.nn.Sequential(\n",
    "        conv_bn(3, 64, kernel_size=3, stride=1, padding=1),\n",
    "        conv_bn(64, 128, kernel_size=5, stride=2, padding=2),\n",
    "        Residual(ch.nn.Sequential(conv_bn(128, 128), conv_bn(128, 128))),\n",
    "        conv_bn(128, 256, kernel_size=3, stride=1, padding=1),\n",
    "        ch.nn.MaxPool2d(2),\n",
    "        Residual(ch.nn.Sequential(conv_bn(256, 256), conv_bn(256, 256))),\n",
    "        conv_bn(256, 128, kernel_size=3, stride=1, padding=0),\n",
    "        ch.nn.AdaptiveMaxPool2d((1, 1)),\n",
    "        Flatten(),\n",
    "        ch.nn.Linear(128, num_classes, bias=False),\n",
    "        Mul(0.2)\n",
    "    )\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e4f18854-2f71-4c42-94ec-a42597e1dfe6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "model = construct_rn9().to(memory_format=ch.channels_last).cuda()\n",
    "model = model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "89886b24-27f2-4876-9e56-a7ed368fe3c0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "loader_train = get_dataloader(batch_size=batch_size, split='train')\n",
    "loader_val = get_dataloader(batch_size=batch_size, split='val')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bf8314-0ca8-4d92-bc13-f3d3b3638852",
   "metadata": {},
   "source": [
    "# Train models"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab3048eb-4bf3-4a8c-b263-2915dd348eb7",
   "metadata": {},
   "source": [
    "We exactly follow the steps in https://docs.ffcv.io/ffcv_examples/cifar10.html, except for the fact that we replace the CIFAR-10 dataloader with the CIFAR-2 dataloader above.\n",
    "\n",
    "We train a total of 100 models."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b385a90-a795-478c-a5b9-83d13d9e2ad4",
   "metadata": {},
   "source": [
    "# Make a list of trained model checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f9b23e24-4a9b-423b-93d3-85e50f264f94",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ckpt_files = list(Path('/path/to/ckpts/dir').rglob('*/*.pt'))\n",
    "ckpts = [ch.load(ckpt, map_location='cpu') for ckpt in ckpt_files]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7ae17a5a-8d63-493b-94b4-41f353dff4f2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "100"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(ckpts)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bbe341a-681c-4ea0-9685-18910e32485c",
   "metadata": {},
   "source": [
    "# Calculate ground-truth model outputs (predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "04d1e013-942a-426f-a6c0-327857c284bb",
   "metadata": {
    "tags": []
   },
   "source": [
    "We exactly follow the steps in https://docs.ffcv.io/ffcv_examples/cifar10.html, except for the fact that we replace the CIFAR-10 dataloader with the CIFAR-2 dataloader above. Additionally, we train on *subsets* of CIFAR-2, parametrized by the `masks` arrays below. We collect the model outputs for each retraining on a different subset (mask) in a separate array `margins`.\n",
    "\n",
    "We train a total of 10,000 models. Note that this is not necessary to get TRAK scores. This step is only necessary to get (very high quality) LDS correlation estimates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6028c404-c13a-4316-88de-4909254a0934",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 9948/9948 [00:02<00:00, 3675.60it/s]\n"
     ]
    }
   ],
   "source": [
    "EVAL_DIR = Path('/path/to/ckpts/for/eval/dir')\n",
    "\n",
    "indices = np.where(np.load(EVAL_DIR / '_completed.npy'))[0]\n",
    "comp_indices = []\n",
    "\n",
    "for i in tqdm(range(0, 99480, 10)):\n",
    "    if all(j in indices for j in range(i,i+10)):\n",
    "        comp_indices.extend(list(range(i,i+10)))\n",
    "\n",
    "masks = {}\n",
    "margins = {}\n",
    "\n",
    "masks['cifar2-avg'] = np.load(EVAL_DIR / 'mask.npy')[comp_indices[::10]]\n",
    "margins['cifar2-avg'] = np.load(EVAL_DIR / 'val_margins.npy')[comp_indices]\n",
    "margins['cifar2-avg'] = margins['cifar2-avg'].reshape(len(margins['cifar2-avg']) // 10,10,2000).mean(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "de01fa22-a882-4884-8edf-0b54221549a9",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9890, 10000)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "masks['cifar2-avg'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "41e75e47-6dd6-4ce1-9957-adad611d7e78",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(9890, 2000)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "margins['cifar2-avg'].shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb6f3682-b142-416f-bf0c-4b0764d41239",
   "metadata": {},
   "source": [
    "# Calcuate TRAK scores for the checkpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6629868-7026-4e9f-bbea-1d69515d6944",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "traker = TRAKer(model=model, task='image_classification', train_set_size=10_000,\n",
    "                proj_dim=4096, save_dir='./trak_results_cifar2',\n",
    "                device='cuda:0')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e59f08f4-5a0d-4756-b530-48fcb53857e7",
   "metadata": {},
   "source": [
    "Note: we can parallelize the steps below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "5a03562a-44eb-4471-9ec7-24651f9f4f33",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 100/100 [18:44<00:00, 11.24s/it]\n",
      "Finalizing features for all model IDs..: 100%|██████████| 100/100 [05:34<00:00,  3.34s/it]\n"
     ]
    }
   ],
   "source": [
    "for model_id, ckpt in enumerate(tqdm(ckpts)):\n",
    "    traker.load_checkpoint(ckpt, model_id=model_id)\n",
    "    for batch in loader_train:\n",
    "        traker.featurize(batch=batch, num_samples=batch_size)\n",
    "\n",
    "traker.finalize_features()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94970e15-aba0-4ad2-953f-95e8ee1e085f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "for model_id, ckpt in enumerate(tqdm(ckpts)):\n",
    "    traker.start_scoring_checkpoint(ckpt, model_id=model_id, num_targets = len(loader_val.indices))\n",
    "    for batch in loader_val:\n",
    "        traker.score(batch=batch, num_samples=batch_size)\n",
    "\n",
    "scores = traker.finalize_scores()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dd8873-e228-4b0b-b4bc-8c1dafafd2f5",
   "metadata": {},
   "source": [
    "# Evaluate the correlation between model outputs and their TRAK estimates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dad49fe2-c9ec-4284-a807-c1ebdc77720c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def eval_correlations(infls, val_inds, masks, margins):\n",
    "    preds = masks @ infls\n",
    "    rs = []\n",
    "    ps = []\n",
    "    for ind, j in tqdm(enumerate(val_inds)):\n",
    "        r, p = spearmanr(preds[:, ind], margins[:, j])\n",
    "        rs.append(r)\n",
    "        ps.append(p)\n",
    "    return np.array(rs), np.array(ps)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12853679-644d-4574-9b3d-0b36dd4813f5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "infls = scores.cpu().numpy()\n",
    "val_inds = np.arange(2000)\n",
    "rs, ps = eval_correlations(infls, val_inds, masks['cifar2-avg'], margins['cifar2-avg'])\n",
    "\n",
    "print(f'Correlation: {rs.mean()} (avg p value {ps.mean()})')\n",
    "plt.hist(rs)\n",
    "plt.savefig('rs.png')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
